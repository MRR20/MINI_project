{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_3 automate everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv ->\n",
    "#   Stock_Open  Stock_close  avg_compound\n",
    "\n",
    "#For weekends ->\n",
    "#   Stock_Open(last_weekday)  Stock_close(last_weekday)  avg_compound(weekend_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    timestamp     open    high     low   close     volume\n",
      "0  2024-08-21  127.315  129.35  126.66  128.50  257883572\n",
      "1  2024-08-20  128.400  129.88  125.89  127.25  300087415\n",
      "2  2024-08-19  124.280  130.00  123.42  130.00  318333577\n",
      "3  2024-08-16  121.940  125.00  121.18  124.58  302589872\n",
      "4  2024-08-15  118.760  123.24  117.47  122.86  318086673\n",
      "6241\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "class StockData:\n",
    "    def __init__(self, stock_name):\n",
    "        self.stock_name = stock_name\n",
    "\n",
    "    def stock_data(self):\n",
    "        api_key = \"COOFAIV9AV9SVV19\"\n",
    "        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&outputsize=full&symbol={self.stock_name}&apikey={api_key}&datatype=csv'\n",
    "        r = requests.get(url)\n",
    "        return r.text\n",
    "\n",
    "sd = StockData(\"NVDA\")\n",
    "data1 = sd.stock_data()\n",
    "df = pd.read_csv(StringIO(data1))\n",
    "\n",
    "print(df.head())\n",
    "print(len(df['close']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       128.50\n",
       "1       127.25\n",
       "2       130.00\n",
       "3       124.58\n",
       "4       122.86\n",
       "         ...  \n",
       "6236     28.25\n",
       "6237     29.19\n",
       "6238     27.44\n",
       "6239     25.00\n",
       "6240     23.50\n",
       "Name: close, Length: 6241, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "now = date.today()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\RUTHVIK\n",
      "[nltk_data]     REDDY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import csv\n",
    "# from io import StringIO\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime, timedelta\n",
    "# import nltk\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# import datetime\n",
    "# from datetime import date, datetime, timedelta\n",
    "\n",
    "# now = date.today()\n",
    "\n",
    "\n",
    "# #NewsScraper Class\n",
    "# class NewsScraper:\n",
    "#     analyzer = SentimentIntensityAnalyzer()\n",
    "#     avg_compound=[]\n",
    "#     epub_date=[]\n",
    "\n",
    "#     #init\n",
    "#     def __init__(self, api_key, total_days):\n",
    "#         self.api_key = api_key\n",
    "#         self.total_days = total_days\n",
    "#         self.now = datetime.now()\n",
    "\n",
    "#     #scrape_new method\n",
    "#     def scrape_news(self):\n",
    "#         for i in range(int(self.total_days)):\n",
    "#             avg=0\n",
    "\n",
    "#             # request\n",
    "#             url = f'https://content.guardianapis.com/search?section=business&page-size=200&from-date={self.now-timedelta(days=i)}&to-date={self.now-timedelta(days=i)}&show-fields=body&api-key={self.api_key}'\n",
    "\n",
    "#             response = requests.get(url)\n",
    "#             data = response.json()\n",
    "\n",
    "#             # article content\n",
    "#             article_content = []\n",
    "#             for article in data['response']['results']:\n",
    "#                 content = article.get('body', '')\n",
    "#                 publication_date = article.get('webPublicationDate', '')\n",
    "#                 date_string = publication_date.split('T')\n",
    "#                 article_content.append(content)\n",
    "#             self.epub_date.append(date_string)\n",
    "\n",
    "#             # sentiment analysis\n",
    "#             sentiment_list = []\n",
    "#             for article in article_content:\n",
    "#                 temp = self.analyzer.polarity_scores(article)\n",
    "#                 sentiment_list.append(temp)\n",
    "\n",
    "#             #collection of compound values\n",
    "#             compound_list=[]\n",
    "#             for i in sentiment_list:\n",
    "#                 temp = i.get('compound')\n",
    "#                 compound_list.append(temp)\n",
    "\n",
    "#             #average of daily compound\n",
    "#             sum=0\n",
    "#             for i in compound_list:\n",
    "#                 sum=sum+i\n",
    "\n",
    "#             avg=sum/(len(compound_list))\n",
    "\n",
    "#             self.avg_compound.append(avg)\n",
    "\n",
    "#             #clear all the variables\n",
    "#             avg=0\n",
    "#             article_content.clear()\n",
    "#             sentiment_list.clear()\n",
    "#             compound_list.clear()\n",
    "#             date_string=\"\"\n",
    "\n",
    "# # usage\n",
    "# api_key = '1e78027b-d07c-4e35-9a0a-8f1d2b4e5549'\n",
    "\n",
    "# start_date='2022-10-01'\n",
    "# current_date=str(now)\n",
    "\n",
    "# d1 = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "# d2 = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "\n",
    "# total_days=d2-d1\n",
    "\n",
    "# ns = NewsScraper(api_key,total_days.days)\n",
    "\n",
    "# ns.scrape_news()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "[-0.3117666666666667, 0.53845, 0.6196818181818181, 0.25249166666666667, 0.09830000000000004, 0.9917499999999999, 0.34463333333333335, 0.8175727272727275, 0.7319333333333332, 0.5815214285714285, 0.5064500000000001, -0.008133333333333326, 0.6578285714285714, 0.2497875, 0.33088333333333336, 0.9644875, -0.11450714285714285, -0.2614888888888889, 0.5108124999999999, 0.9907666666666667, 0.33273749999999996]\n",
      "['2024-08-22', '2024-08-21', '2024-08-20', '2024-08-19', '2024-08-18', '2024-08-17', '2024-08-16', '2024-08-15', '2024-08-14', '2024-08-13', '2024-08-12', '2024-08-11', '2024-08-10', '2024-08-09', '2024-08-08', '2024-08-07', '2024-08-06', '2024-08-05', '2024-08-04', '2024-08-03', '2024-08-02']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "class NewsScraper:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    avg_compound = []\n",
    "    epub_date = []\n",
    "\n",
    "    def __init__(self, api_key, total_days):\n",
    "        self.api_key = api_key\n",
    "        self.total_days = total_days\n",
    "        self.now = datetime.now()\n",
    "\n",
    "    def scrape_news(self):\n",
    "        c=0\n",
    "        for i in range(self.total_days):\n",
    "            avg = 0\n",
    "            epub_date_day = []\n",
    "\n",
    "            date = (self.now - timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "            url = f'https://content.guardianapis.com/search?section=business&page-size=200&from-date={date}&to-date={date}&show-fields=body&api-key={self.api_key}'\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                data = response.json()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error requesting API: {e}\")\n",
    "                continue\n",
    "\n",
    "            if 'response' not in data or 'results' not in data.get('response', {}):\n",
    "                print(f\"Error: Unexpected response format. Response: {data}\")\n",
    "                continue\n",
    "\n",
    "            article_content = []\n",
    "            for article in data['response']['results']:\n",
    "                content = article.get('fields', {}).get('body', '')\n",
    "                publication_date = article.get('webPublicationDate', '')\n",
    "                date_string = publication_date.split('T')[0]  # extract the date part\n",
    "\n",
    "                # Remove HTML tags using BeautifulSoup\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                content = soup.get_text()\n",
    "\n",
    "                article_content.append(content)\n",
    "                epub_date_day.append(date_string)  # append the date string\n",
    "\n",
    "            sentiment_list = []\n",
    "            for article in article_content:\n",
    "                temp = self.analyzer.polarity_scores(article)\n",
    "                sentiment_list.append(temp)\n",
    "\n",
    "            compound_list = []\n",
    "            for i in sentiment_list:\n",
    "                temp = i.get('compound')\n",
    "                compound_list.append(temp)\n",
    "\n",
    "            if compound_list:\n",
    "                sum = 0\n",
    "                for i in compound_list:\n",
    "                    sum = sum + i\n",
    "\n",
    "                avg = sum / (len(compound_list))\n",
    "\n",
    "                self.avg_compound.append(avg)\n",
    "                self.epub_date.append(epub_date_day[0])\n",
    "\n",
    "            avg = 0\n",
    "            article_content.clear()\n",
    "            sentiment_list.clear()\n",
    "            compound_list.clear()\n",
    "            epub_date_day.clear()\n",
    "\n",
    "            c=c+1\n",
    "            print(c)\n",
    "\n",
    "api_key = '1e78027b-d07c-4e35-9a0a-8f1d2b4e5549'\n",
    "\n",
    "start_date = '2024-08-01'\n",
    "current_date = str(date.today())\n",
    "\n",
    "d1 = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "d2 = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "\n",
    "total_days = (d2 - d1).days\n",
    "\n",
    "ns = NewsScraper(api_key, total_days)\n",
    "ns.scrape_news()\n",
    "\n",
    "print(ns.avg_compound)\n",
    "print(ns.epub_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(ns.avg_compound))\n",
    "print(len(ns.epub_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from io import StringIO\n",
    "# df = pd.read_csv(StringIO(data1))\n",
    "\n",
    "# print(df.head())\n",
    "# print(len(df['close']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import datetime class from datetime module\n",
    "# from datetime import date, datetime, timedelta\n",
    "\n",
    "# # returns current date and time\n",
    "# now = date.today()\n",
    "# print(now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date='2022-10-01'\n",
    "# current_date=str(now)\n",
    "\n",
    "# d1 = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "# d2 = datetime.strptime(current_date, \"%Y-%m-%d\")\n",
    "\n",
    "# total_days=d2-d1\n",
    "\n",
    "# print(total_days.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import csv\n",
    "# from io import StringIO\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# api_key = '1e78027b-d07c-4e35-9a0a-8f1d2b4e5549'\n",
    "\n",
    "# for i in range(total_days.days):\n",
    "#     avg=0\n",
    "\n",
    "#     #request\n",
    "#     url = f'https://content.guardianapis.com/search?section=business&page-size=200&from-date={now-timedelta(days=i)}&to-date={now-timedelta(days=i)}&show-fields=body&api-key={api_key}'\n",
    "\n",
    "#     response = requests.get(url)\n",
    "#     data=response.json()\n",
    "\n",
    "#     #article content\n",
    "#     article_content=[]\n",
    "\n",
    "#     for article in data['response']['results']:\n",
    "#         content = article.get('body', '')\n",
    "#         article_content.append(content)\n",
    "\n",
    "#     #sentiment analysis\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('monthly_milk_production.csv',\n",
    "# \t\t\t\tindex_col='Date',\n",
    "# \t\t\t\tparse_dates=True)\n",
    "# df.index.freq = 'MS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting graph b/w production and date\n",
    "# df.plot(figsize=(12, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# results = seasonal_decompose(df['Production'])\n",
    "# results.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df.iloc[:156]\n",
    "# test = df.iloc[156:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(train)\n",
    "# scaled_train = scaler.transform(train)\n",
    "# scaled_test = scaler.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# n_input = 3\n",
    "# n_features = 1\n",
    "# generator = TimeseriesGenerator(scaled_train,\n",
    "# \t\t\t\t\t\t\t\tscaled_train,\n",
    "# \t\t\t\t\t\t\t\tlength=n_input,\n",
    "# \t\t\t\t\t\t\t\tbatch_size=1)\n",
    "# X, y = generator[0]\n",
    "# print(f'Given the Array: \\n{X.flatten()}')\n",
    "# print(f'Predict this y: \\n {y}')\n",
    "# # We do the same thing, but now instead for 12 months\n",
    "# n_input = 12\n",
    "# generator = TimeseriesGenerator(scaled_train,\n",
    "# \t\t\t\t\t\t\t\tscaled_train,\n",
    "# \t\t\t\t\t\t\t\tlength=n_input,\n",
    "# \t\t\t\t\t\t\t\tbatch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(100, activation='relu',\n",
    "# \t\t\tinput_shape=(n_input, n_features)))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.summary()\n",
    "# model.fit(generator, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
